{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Q-learning in Python\n",
    "\n",
    "__PyData Bristol__ meetup, 17 July 2019. Author: Jan Malte Lichtenberg (j.m.lichtenberg@bath.ac.uk)\n",
    "\n",
    "Implement tabular Q-learning algorithm in a simple \"grid world\" environment and visualise learning behaviour. The following figure shows the agent-environment interaction (figure from Sutton & Barto 1998).\n",
    "<div>\n",
    "<div>\n",
    "<img src=\"images/withAgent.png\" style=\"width: 30%;\" align=\"left\"/> \n",
    "</div>\n",
    "<div>\n",
    "<img src=\"images/agent-environment.png\" style=\"width: 55%;\"/> \n",
    "</div>\n",
    "</div>\n",
    "We will need an __agent__, an __environment__, and a procedure that determines how these two entities interact with each other. The environment is provided, the rest will be implemented as we go along..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment: a \"grid world\" \n",
    "\n",
    "The following cell imports the `Gridworld` class from the `gridowrld.py` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gridworld\n",
    "env = gridworld.Gridworld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent starts in a randomly chosen position in the bottom row. We can visualize the current state of the environment as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "env.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Gridworld` class has a method called `make_step()` that takes an `action_index` as an input and returns a new state as well as the reward associated with the step. The agent can choose among four actions (`0 = \"north\", 1 = \"east\", 2 = \"south\", 3 = \"west\"`).  Let's go north and visualize the environment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.make_step(action_index=0)  #  0 = \"north\",   1 = \"east\",   2 = \"south\",   3 = \"west\"\n",
    "env.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, new_position = env.make_step(action_index=0)  #  0 = \"north\",   1 = \"east\",   2 = \"south\",   3 = \"west\"\n",
    "env.visualize(show_state_labels=True)\n",
    "print(\"The agent's new position is\", new_position)\n",
    "print(\"The obtained reward is \", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-learning agent.\n",
    "\n",
    "<div>\n",
    "<div>\n",
    "<img src=\"images/q.png\" style=\"width: 66.65%;\" align=\"left\"/>\n",
    "</div>\n",
    "<div>\n",
    "<img src=\"images/agent-environment.png\" style=\"width: 33.33%;\" align=\"left\"/> \n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Figures from Sutton & Barto (1998), Reinforcement Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def q_learning(env, agent, num_episodes):\n",
    "#     for each episode:\n",
    "#        initialize state\n",
    "#        for each step in episode:\n",
    "#           action = agent.choose_action(old_state)\n",
    "#           reward, new_state = env.make_step(action)\n",
    "#           agent.learn(old_state, action, reward, new_state)\n",
    "#           old_state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AgentQ:\n",
    "    def __init__(self, ...):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn for a couple of episodes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gridworld.Gridworld()\n",
    "agent = AgentQ(num_states=env.num_states, num_actions=env.num_actions)\n",
    "q_learning(env=env, agent=agent, num_episodes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of policy and learning curve for a Q-learning agent\n",
    "We now let the q-learning agent play and learn (without watching) and then visualize its policy afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from q_solution import AgentQ, q_learning\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gridworld.Gridworld(random_move_probability=0.01)\n",
    "agent = AgentQ(env)\n",
    "\n",
    "# Do not visualize during play.\n",
    "reward_per_episode = q_learning(env=env, agent=agent, num_episodes=500)\n",
    "\n",
    "# Plot policy and learning curve.\n",
    "env.visualize(show_grid=True, show_policy=True, show_learning_curve=True,\n",
    "              episode=len(reward_per_episode), reward_per_episode=reward_per_episode, \n",
    "              agent_q_table=agent.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization of learning process for a Q-learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Let agentQ play and watch while she is learning\n",
    "np.random.seed(2)\n",
    "\n",
    "# Define the envronment\n",
    "env = gridworld.Gridworld(random_move_probability=0.01, bomb_positions=np.array([13]))\n",
    "\n",
    "# Define the Q-learning agent\n",
    "agent = AgentQ(env, alpha=0.1, epsilon=0.01)\n",
    "\n",
    "rewards_per_episode = q_learning(env, agent, num_episodes=500, learn=True,\n",
    "                                 show_grid=True, show_q_values=True, show_learning_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
